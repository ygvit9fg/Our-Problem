import os
from openai import OpenAI
from transformers import AutoTokenizer, AutoModelForCausalLM
import torch


client = OpenAI(api_key=os.getenv("OPENAI_API_KEY"))


LOCAL_MODEL = "gpt2-large"
device = "cuda" if torch.cuda.is_available() else "cpu"

print(f"–ó–∞–≥—Ä—É–∑–∫–∞ –ª–æ–∫–∞–ª—å–Ω–æ–π –º–æ–¥–µ–ª–∏ {LOCAL_MODEL}...")
tokenizer = AutoTokenizer.from_pretrained(LOCAL_MODEL)
model = AutoModelForCausalLM.from_pretrained(
    LOCAL_MODEL,
    torch_dtype=torch.float32,   # –ª—É—á—à–µ –¥–ª—è —Å—Ç–∞–±–∏–ª—å–Ω–æ—Å—Ç–∏
).to(device)  # üöÄ —è–≤–Ω–æ –≥—Ä—É–∑–∏–º –Ω–∞ CPU –∏–ª–∏ GPU


def ask_online(prompt: str) -> str:

    response = client.chat.completions.create(
        model="gpt-4o-mini",
        messages=[
            {"role": "system", "content": "–¢—ã —É–º–Ω—ã–π –ø–æ–º–æ—â–Ω–∏–∫."},
            {"role": "user", "content": prompt},
        ]
    )
    return response.choices[0].message.content

def ask_offline(prompt: str) -> str:
    inputs = tokenizer(prompt, return_tensors="pt").to(device)

    with torch.no_grad():
        outputs = model.generate(
            **inputs,
            max_new_tokens=150,
            pad_token_id=tokenizer.eos_token_id,
            do_sample=True,
            top_k=50,
            top_p=0.95,
            temperature=0.7
        )

    generated_text = tokenizer.decode(outputs[0], skip_special_tokens=True)

    # üîé –û—Ç–ª–∞–¥–∫–∞: –ø–æ–∫–∞–∑—ã–≤–∞–µ–º –≤–µ—Å—å —Ç–µ–∫—Å—Ç
    print("\n[DEBUG] –°—ã—Ä–æ–π –æ—Ç–≤–µ—Ç –º–æ–¥–µ–ª–∏:")
    print(generated_text)

    clean_answer = generated_text[len(prompt):].strip()
    print("\n[OFFLINE] –û—Ç–≤–µ—Ç –º–æ–¥–µ–ª–∏:")
    print(clean_answer)
    return clean_answer




if __name__ == "__main__":
    # –¢–µ—Å—Ç–∏—Ä—É–µ–º –≥–µ–Ω–µ—Ä–∞—Ü–∏—é –Ω–∞–ø—Ä—è–º—É—é
    inputs = tokenizer("–ü—Ä–∏–≤–µ—Ç! –†–∞—Å—Å–∫–∞–∂–∏ –ø—Ä–æ –ò—Ç–∞–ª–∏—é.", return_tensors="pt").to(device)
    outputs = model.generate(**inputs, max_new_tokens=50)
    print("\n[TEST] –ì–µ–Ω–µ—Ä–∞—Ü–∏—è –Ω–∞–ø—Ä—è–º—É—é:")
    print(tokenizer.decode(outputs[0], skip_special_tokens=True))


if __name__ == "__main__":
    mode = input("–í—ã–±–µ—Ä–∏ —Ä–µ–∂–∏–º (online/offline): ").strip().lower()

    while True:
        question = input("\n‚ùì –í–æ–ø—Ä–æ—Å (–∏–ª–∏ 'exit' –¥–ª—è –≤—ã—Ö–æ–¥–∞): ")

        if question.lower() in ["exit", "quit", "–≤—ã—Ö–æ–¥"]:
            print("üëã –í—ã—Ö–æ–¥ –∏–∑ –ø—Ä–æ–≥—Ä–∞–º–º—ã.")
            break

        if mode == "online":
            print("\nüåê –û–Ω–ª–∞–π–Ω —Ä–µ–∂–∏–º:")
            print(ask_online(question))

        elif mode == "offline":
            print("\nüíª –û—Ñ—Ñ–ª–∞–π–Ω —Ä–µ–∂–∏–º:")
            print(ask_offline(question))

        else:
            print("‚ö†Ô∏è –ù–µ–≤–µ—Ä–Ω—ã–π –≤—ã–±–æ—Ä! –ù–∞–ø–∏—à–∏ 'online' –∏–ª–∏ 'offline'.")
            break

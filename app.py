import os
from openai import OpenAI
from transformers import AutoTokenizer, AutoModelForCausalLM
import torch


client = OpenAI(api_key=os.getenv("OPENAI_API_KEY"))


LOCAL_MODEL = "gpt2-large"
device = "cuda" if torch.cuda.is_available() else "cpu"

print(f"–ó–∞–≥—Ä—É–∑–∫–∞ –ª–æ–∫–∞–ª—å–Ω–æ–π –º–æ–¥–µ–ª–∏ {LOCAL_MODEL}...")
tokenizer = AutoTokenizer.from_pretrained(LOCAL_MODEL)
model = AutoModelForCausalLM.from_pretrained(
    LOCAL_MODEL,
    torch_dtype=torch.float16 if torch.cuda.is_available() else torch.float32
).to(device)

def ask_online(prompt: str) -> str:

    response = client.chat.completions.create(
        model="gpt-4o-mini",
        messages=[
            {"role": "system", "content": "–¢—ã —É–º–Ω—ã–π –ø–æ–º–æ—â–Ω–∏–∫."},
            {"role": "user", "content": prompt},
        ]
    )
    return response.choices[0].message.content

def ask_offline(prompt: str) -> str:

    inputs = tokenizer(prompt, return_tensors="pt").to(device)
    with torch.no_grad():
        outputs = model.generate(
            **inputs,
            max_new_tokens=200,
            do_sample=True,
            top_k=50,
            top_p=0.95,
            temperature=0.8
        )
    text = tokenizer.decode(outputs[0], skip_special_tokens=True)
    return text[len(prompt):].strip()


if __name__ == "__main__":
    question = "–ü—Ä–∏–≤–µ—Ç, —Ä–∞—Å—Å–∫–∞–∂–∏ –∏–Ω—Ç–µ—Ä–µ—Å–Ω—ã–π —Ñ–∞–∫—Ç –ø—Ä–æ –∫–æ—Å–º–æ—Å!"

    mode = input("–í—ã–±–µ—Ä–∏ —Ä–µ–∂–∏–º (online/offline): ").strip().lower()

    if mode == "online":
        print("üåê –û–Ω–ª–∞–π–Ω —Ä–µ–∂–∏–º:")
        print(ask_online(question))

    elif mode == "offline":
        print("üíª –û—Ñ—Ñ–ª–∞–π–Ω —Ä–µ–∂–∏–º:")
        print(ask_offline(question))

    else:
        print("‚ö†Ô∏è –ù–µ–≤–µ—Ä–Ω—ã–π –≤—ã–±–æ—Ä! –ù–∞–ø–∏—à–∏ 'online' –∏–ª–∏ 'offline'.")
